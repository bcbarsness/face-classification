{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gender & Emotion Classification\n",
    "    Source: https://github.com/oarriaga/face_classification\n",
    "The success of service robotics decisively depends on a smooth robot to user interaction. Thus, a robot should be\n",
    "able to extract information just from the face of its user, e.g. identify the emotional state or deduce gender. Interpreting\n",
    "correctly any of these elements using machine learning (ML) techniques has proven to be complicated due the high\n",
    "variability of the samples within each task. This leads to models with millions of parameters trained under thousands of\n",
    "samples. Furthermore, the human accuracy for classifying an image of a face in one of 7 different emotions(“angry”, “disgust”, “fear”,“happy”, “sad”, “surprise”, “neutral”) is 65% +/-5%. \n",
    "\n",
    "In spite of these difficulties, robot platforms oriented to attend and solve household tasks require facial expressions\n",
    "systems that are robust and computationally efficient. Moreover, the state-of-the-art methods in image-related tasks such\n",
    "as image classification and object detection are all based on Convolutional Neural Networks (CNNs).\n",
    "\n",
    "In this demo we explore the application of Neural Networks in Machine Learning through the following hands-on exercises:\n",
    "\n",
    "a) Gender and emotion classification on image files.                                                                             \n",
    "b) Real-time vision system which accomplishes the tasks of face detection, gender classification and emotion classification\n",
    "\n",
    " ![title](../images/demo_results.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hands On Exercises\n",
    "**a)** **Gender and emotion classification on image files.**\n",
    "   \n",
    "   All the input images and output images can be found in the images folder highlighted below:                                    ![title](../images/FolderImage.PNG)\n",
    "   \n",
    "   If your input file name is **test_image.jpg** then the output file name after model run is **predicted_test_image.jpg**\n",
    "   \n",
    "  ![title](../images/testInOut.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code below is what is used to execute the image classification on **test_image.jpg**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i image_emotion_gender_demo.py ../images/test_image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1: See if you can download a good image file of your own on the web or your DPN profile picture and save it in the \n",
    "#images folder\n",
    "\n",
    "#All you'd need to run the classification is the name of your image file and replace ???.??? below with your filename together \n",
    "#with its extension.\n",
    "\n",
    "#Click the RUN button at the top of the kernel...wait 20 - 30s to check for your predicted output in the images folder.\n",
    "\n",
    "%run -i image_emotion_gender_demo.py ../images/???.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** **Real-time webcam emotion classification**\n",
    "   \n",
    "  ![title](../images/color_demo.gif)\n",
    "  \n",
    "  To **RUN** the real time webcam classification, put your cursor in the code below and hit the RUN button up top.\n",
    "  \n",
    "  To **EXIT** hit the **STOP** button up top to interrupt the kernel and then close the webcam pop up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Execute the code below for a real-time webcam classification:\n",
    "%run -i video_emotion_color_demo.py\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** **Producing the Important Features File**.\n",
    "   These files enable us to view how the computer is classifying the images. If there are biases that are causing misclassifications, we can also be able to visualize them.\n",
    "   \n",
    "   This file is as highlighted below:\n",
    "   ![title](../images/Ex3.PNG)\n",
    "   \n",
    "   It is also the important features file depicted on the right hand image below:\n",
    "   ![title](../images/gradcam_results.png)\n",
    "   \n",
    "   The code needed to produce the file is below. The name of the output file in the images folder is always **guided_gradCAM.png**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i image_gradcam_demo.py ../images/test_image.jpg\n",
    "\n",
    "#Exercise 3: See if you can  substitute 'test_image.jpg' with a file of your choosing within the image folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
