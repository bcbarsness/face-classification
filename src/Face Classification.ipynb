{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gender & Emotion Classification\n",
    "    Source: https://github.com/oarriaga/face_classification\n",
    "The success of service robotics decisively depends on a smooth robot to user interaction. Thus, a robot should be\n",
    "able to extract information just from the face of its user, e.g. identify the emotional state or deduce gender. Interpreting\n",
    "correctly any of these elements using machine learning (ML) techniques has proven to be complicated due the high\n",
    "variability of the samples within each task. This leads to models with millions of parameters trained under thousands of\n",
    "samples. Furthermore, the human accuracy for classifying an image of a face in one of 7 different emotions(“angry”, “disgust”, “fear”,“happy”, “sad”, “surprise”, “neutral”) is 65% +/-5%. \n",
    "\n",
    "In spite of these difficulties, robot platforms oriented to attend and solve household tasks require facial expressions\n",
    "systems that are robust and computationally efficient. Moreover, the state-of-the-art methods in image-related tasks such\n",
    "as image classification and object detection are all based on Convolutional Neural Networks (CNNs).\n",
    "\n",
    "In this demo we explore the application of Neural Networks in Machine Learning through the following hands-on exercises:\n",
    "\n",
    "a) Gender and emotion classification on image files.                                                                             \n",
    "b) Real-time vision system which accomplishes the tasks of face detection, gender classification and emotion classification\n",
    "\n",
    " ![title](../images/demo_results.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hands On Exercises\n",
    "**a)** **Gender and emotion classification on image files.**\n",
    "   \n",
    "   All the input images and output images can be found in the images folder highlighted below:                                    ![title](../images/FolderImage.PNG)\n",
    "   \n",
    "   If your input file name is **test_image.jpg** then the output file name after model run is **predicted_test_image.jpg**\n",
    "   \n",
    "  ![title](../images/testInOut.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code below is what is used to execute the image classification on **test_image.jpg**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imread'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m\\\\Mac\\Home\\Desktop\\Projects\\ML Guild\\Final Files\\Guild Advisory Demo Files\\face_classification-master\\src\\image_emotion_gender_demo.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_detection_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# parameters for loading data and images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m\\\\Mac\\Home\\Desktop\\Projects\\ML Guild\\Final Files\\Guild Advisory Demo Files\\face_classification-master\\src\\utils\\preprocessor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimresize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'imread'"
     ]
    }
   ],
   "source": [
    "%run -i image_emotion_gender_demo.py ../images/test_image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1: See if you can download a good image file of your own on the web or your DPN profile picture and save it in the \n",
    "#images folder\n",
    "\n",
    "#All you'd need to run the classification is the name of your image file and replace ???.??? below with your filename together \n",
    "#with its extension.\n",
    "\n",
    "#Click the RUN button at the top of the kernel...wait 20 - 30s to check for your predicted output in the images folder.\n",
    "\n",
    "%run -i image_emotion_gender_demo.py ../images/???.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** **Real-time webcam emotion classification**\n",
    "   \n",
    "  ![title](../images/color_demo.gif)\n",
    "  \n",
    "  To **RUN** the real time webcam classification, put your cursor in the code below and hit the RUN button up top.\n",
    "  \n",
    "  To **EXIT** hit the **STOP** button up top to interrupt the kernel and then close the webcam pop up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\cmasunda\\AppData\\Local\\Continuum\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\cmasunda\\AppData\\Local\\Continuum\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1190: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\cmasunda\\AppData\\Local\\Continuum\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1297: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Execute the code below for a real-time webcam classification:\n",
    "%run -i video_emotion_color_demo.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** **Producing the Important Features File**.\n",
    "   These files enable us to view how the computer is classifying the images. If there are biases that are causing misclassifications, we can also be able to visualize them.\n",
    "   \n",
    "   This file is as highlighted below:\n",
    "   ![title](../images/Ex3.PNG)\n",
    "   \n",
    "   It is also the important features file depicted on the right hand image below:\n",
    "   ![title](../images/gradcam_results.png)\n",
    "   \n",
    "   The code needed to produce the file is below. The name of the output file in the images folder is always **guided_gradCAM.png**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i image_gradcam_demo.py ../images/test_image.jpg\n",
    "\n",
    "#Exercise 3: See if you can  substitute 'test_image.jpg' with a file of your choosing within the image folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
